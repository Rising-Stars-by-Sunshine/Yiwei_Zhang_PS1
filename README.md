# Data Descriptor
# DICES Dataset: Diversity in Conversational AI Evaluation for Safety
# Background
Machine learning approaches are often trained and evaluated with datasets that require a clear separation between positive and negative examples. This approach overly simplifies the natural subjectivity present in many tasks and content items. It also obscures the inherent diversity in human perceptions and opinions. Often tasks that attempt to preserve the variance in content and diversity in humans are quite expensive and laborious. To fill in this gap and facilitate more in-depth model performance analyses we propose the DICES dataset - a unique dataset with diverse perspectives on safety of AI generated conversations. We focus on the task of safety evaluation of conversational AI systems. The DICES dataset contains detailed demographics information about each rater, extremely high replication of unique ratings per conversation to ensure statistical significance of further analyses and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different rating aggregation strategies.

This dataset is well suited to observe and measure variance, ambiguity and diversity in the context of safety of conversational AI. The dataset is accompanied by a paper describing a set of metrics that show how rater diversity influences the safety perception of raters from different geographic regions, ethnicity groups, age groups and genders. The goal of the DICES datasetis to be used as a shared benchmark for safety evaluation of conversational AIsystems.

# Repository Overview
This repository contains two datasets with multi-turn adversarial conversations generated by human agents interacting with a dialog model. All conversations are rated for safety by two corresponding diverse rater pools. Details for all safety ratings can be found in the corresponding README.md files.

Dataset 990: 990/diverse_safety_adversarial_dialog_990.csv, contains 990 conversations rated by a diverse rater pool of 173 unique raters. Each conversation is rated with three safety top-level categories and one overall conversation comprehension question. Raters were recruited so that the number of raters for each conversation was balanced by gender (Man, Woman) and locale (US, India). Each rater rated only a sample of the conversation. Each conversation has 60-70 unique ratings. Total number of rows in this dataset is 72103.

Dataset 350: 350/diverse_safety_adversarial_dialog_350.csv, contains 350 conversations rated by a diverse rater pool of 123 unique raters. Each conversation is rated with five safety top-level categories and one overall comprehension question of the conversation. Raters were recruited were balanced by gender (man or woman), race/ethnicity (White, Black, Latine, Asian, Multiracial) and each rater rated all items. Each rater rated all conversations. Each conversation has 123 unique ratings. Total number of rows in this dataset is 43050.

<img width="734" alt="Êà™Â±è2025-01-19 00 28 23" src="https://github.com/user-attachments/assets/5d9921ee-e14c-4e4a-8774-2a91bac2043b" />

# üñ•Ô∏è System Configuration Instructions

This repository contains the system configuration details, including **CPU, Memory, Disk Space, Python environment, and installed packages**. These specifications ensure compatibility and reproducibility for research and development.

## ‚öôÔ∏è **System Specifications**
### **CPU Information**
- **Architecture:** x86_64
- **CPU Model:** Intel(R) Xeon(R) CPU @ 2.20GHz
- **Cores:** 1 socket, 2 CPUs, 2 threads per core
- **Virtualization:** Full (KVM)
- **Cache:**
  - **L1:** 32 KiB
  - **L2:** 256 KiB
  - **L3:** 55 MiB

### **Memory Information**
- **Total RAM:** 12 GiB
- **Available:** 11 GiB
- **Swap Memory:** Not enabled

### **Disk Space**
| Filesystem | Size | Used | Available | Usage % |
|------------|------|------|-----------|---------|
| Overlay    | 108G | 32G  | 77G       | 29%     |
| /dev/sda1  | 76G  | 56G  | 21G       | 74%     |

### **GPU Information**
- üö´ **No GPU detected** (Runs on CPU)

---

## üêç **Python Environment**
- **Python Version:** `3.11.11`
- **Package Manager:** `pip`
- **Installed Packages:** (`pip list`)
  
### **üì¶ Key Installed Packages**
| Package | Version |
|---------|---------|
| `numpy` | 1.26.4 |
| `pandas` | 2.2.2 |
| `scikit-learn` | 1.6.0 |
| `tensorflow` | 2.17.1 |
| `torch` | 2.5.1+cu121 |
| `transformers` | 4.47.1 |
| `matplotlib` | 3.10.0 |
| `seaborn` | 0.13.2 |


---

## üöÄ **How to Use**
### **üìå Setup Environment**
To replicate this system environment, follow these steps:

1Ô∏è‚É£ **Clone Repository**  
```bash
git clone https://github.com/Rising-Stars-by-Sunshine/Yiwei_Zhang_PS1.git
cd Yiwei_Zhang_PS1
```
2Ô∏è‚É£ **üìå Create a Virtual Environment**
A **virtual environment** helps manage dependencies **without affecting global Python installations**.

#### **For Linux & macOS:**
```bash
# Check if Python is installed
python3 --version  

# Install Python if not available
sudo apt install python3 python3-venv python3-pip  # Ubuntu/Debian
brew install python                                # macOS

# Create a virtual environment
python3 -m venv venv  

# Activate the virtual environment
source venv/bin/activate  

# Deactivate when done
deactivate
```
#### **For Windows:**
```bash
# Check if Python is installed
python --version  

# Create a virtual environment
python -m venv venv  

# Activate the virtual environment (PowerShell)
venv\Scripts\Activate.ps1  

# Activate in Command Prompt (cmd.exe)
venv\Scripts\activate  

# Deactivate when done
deactivate
```
3Ô∏è‚É£ **Install Dependencies**
Once the virtual environment is activated, install all required Python packages:

```bash
pip install numpy pandas scikit-learn tensorflow torch transformers matplotlib seaborn \
    requests beautifulsoup4 Flask opencv-python scipy nltk tqdm Pillow \
    PyYAML jsonschema protobuf grpcio google-auth google-auth-oauthlib google-cloud-storage \
    google-cloud-bigquery google-cloud-firestore google-cloud-functions google-cloud-translate \
    google-api-python-client google-auth-httplib2 jupyter jupyterlab notebook ipython \
    ipykernel ipywidgets fastai datasets sentencepiece evaluate tensorboard xgboost \
    plotly openai langchain huggingface-hub fastapi uvicorn typer pydantic rich \
    networkx spacy gensim sympy pytest pylint mypy black isort flake8 autopep8
```
4Ô∏è‚É£ **Verify Installation**
```bash
pip list  
python --version
```
5Ô∏è‚É£ **Running the Project**
```bash
python EDA-2.ipynb
```
